coeffs_mp[coeffs.mp>0]
(coeffs_sorted <- sort(coeffs, decreasing = T))
coeffs_mp <- coeffs.sorted[grep('ManufacturingProcess', names(coeffs_sorted))]
coeffs_mp[coeffs_mp>0]
coeffs_sorted <- abs(coeffs)
coeffs_sorted <- coeffs_sorted[coeffs.sorted>0]
(coeffs_sorted <- sort(coeffs_sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
coeffs_mp <- coeffs_sorted[grep('ManufacturingProcess', names(coeffs_sorted))] %>% names() %>% coeffs[.]
coeffs.sorted <- abs(coeffs)
coeffs.sorted <- coeffs.sorted[coeffs.sorted>0]
(coeffs.sorted <- sort(coeffs.sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
coeffs_mp <- coeffs_sorted[grep('ManufacturingProcess', names(coeffs_sorted))] %>% names() %>% coeffs[.]
library(dplyr)
coeffs_mp <- coeffs_sorted[grep('ManufacturingProcess', names(coeffs_sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs.mp>0]
library(dplyr)
coeffs_mp <- coeffs_sorted[grep('ManufacturingProcess', names(coeffs_sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs_mp>0]
coeffs.mp[coeffs.mp<0]
coeffs_mp[coeffs.mp<0]
coeffs_mp[coeffs_mp<0]
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
str(ChemicalManufacturingProcess)
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
str(ChemicalManufacturingProcess)
library(Amelia)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
(cmpImpute <- preProcess(ChemicalManufacturingProcess[,-c(1)], method=c('bagImpute')))
cmp <- predict(cmpImpute, ChemicalManufacturingProcess[,-c(1)])
set.seed(17)
trainRow <- createDataPartition(ChemicalManufacturingProcess$Yield, p=0.8, list=FALSE)
x_train <- cmp[trainRow, ]
y_train <- ChemicalManufacturingProcess$Yield[trainRow]
x_test <- cmp[-trainRow, ]
y_test <- ChemicalManufacturingProcess$Yield[-trainRow]
set.seed(1)
enetFit <- train(x=x_train,
y=y_train,
method='enet',
metric='RMSE',
tuneGrid=expand.grid(.fraction = seq(0, 1, by=0.1),
.lambda = seq(0, 1, by=0.1)),
trControl=trainControl(method='cv'),
preProcess=c('center','scale')
)
enetFit
plot(enetFit)
enet_Pred <- predict(enetFit, newdata=x_test)
(predResult <- postResample(pred=enet_Pred, obs=y_test))
library(elasticnet)
coeffs <- predict.enet(enetFit$finalModel, s=enetFit$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients
coeffs
coeffs.sorted <- abs(coeffs)
coeffs.sorted <- coeffs.sorted[coeffs.sorted>0]
(coeffs.sorted <- sort(coeffs.sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
library(dplyr)
coeffs_mp <- coeffs.sorted[grep('ManufacturingProcess', names(coeffs.sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs_mp>0]
coeffs_mp[coeffs_mp<0]
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
str(ChemicalManufacturingProcess)
library(Amelia)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
(cmpImpute <- preProcess(ChemicalManufacturingProcess[,-c(1)], method=c('bagImpute')))
cmp <- predict(cmpImpute, ChemicalManufacturingProcess[,-c(1)])
set.seed(43)
trainRow <- createDataPartition(ChemicalManufacturingProcess$Yield, p=0.8, list=FALSE)
x_train <- cmp[trainRow, ]
y_train <- ChemicalManufacturingProcess$Yield[trainRow]
x_test <- cmp[-trainRow, ]
y_test <- ChemicalManufacturingProcess$Yield[-trainRow]
set.seed(43)
enetFit <- train(x=x_train,
y=y_train,
method='enet',
metric='RMSE',
tuneGrid=expand.grid(.fraction = seq(0, 1, by=0.1),
.lambda = seq(0, 1, by=0.1)),
trControl=trainControl(method='cv'),
preProcess=c('center','scale')
)
enetFit
plot(enetFit)
enet_Pred <- predict(enetFit, newdata=x_test)
(predResult <- postResample(pred=enet_Pred, obs=y_test))
library(elasticnet)
coeffs <- predict.enet(enetFit$finalModel, s=enetFit$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients
coeffs
coeffs.sorted <- abs(coeffs)
coeffs.sorted <- coeffs.sorted[coeffs.sorted>0]
(coeffs.sorted <- sort(coeffs.sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
library(dplyr)
coeffs_mp <- coeffs.sorted[grep('ManufacturingProcess', names(coeffs.sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs_mp>0]
coeffs_mp[coeffs_mp<0]
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
str(ChemicalManufacturingProcess)
library(Amelia)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
(cmpImpute <- preProcess(ChemicalManufacturingProcess[,-c(1)], method=c('bagImpute')))
cmp <- predict(cmpImpute, ChemicalManufacturingProcess[,-c(1)])
set.seed(43)
trainRow <- createDataPartition(ChemicalManufacturingProcess$Yield, p=0.8, list=FALSE)
x_train <- cmp[trainRow, ]
y_train <- ChemicalManufacturingProcess$Yield[trainRow]
x_test <- cmp[-trainRow, ]
y_test <- ChemicalManufacturingProcess$Yield[-trainRow]
set.seed(43)
enetFit <- train(x=x_train,
y=y_train,
method='enet',
metric='RMSE',
tuneGrid=expand.grid(.fraction = seq(0, 1, by=0.1),
.lambda = seq(0, 1, by=0.1)),
trControl=trainControl(method='cv'),
preProcess=c('center','scale')
)
enetFit
plot(enetFit)
enet_Pred <- predict(enetFit, newdata=x_test)
(predResult <- postResample(pred=enet_Pred, obs=y_test))
library(elasticnet)
coeffs <- predict.enet(enetFit$finalModel, s=enetFit$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients
coeffs
coeffs.sorted <- abs(coeffs)
coeffs.sorted <- coeffs.sorted[coeffs.sorted>0]
(coeffs.sorted <- sort(coeffs.sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
library(dplyr)
coeffs_mp <- coeffs.sorted[grep('ManufacturingProcess', names(coeffs.sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs_mp>0]
coeffs_mp[coeffs_mp<0]
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
str(ChemicalManufacturingProcess)
library(Amelia)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
(cmpImpute <- preProcess(ChemicalManufacturingProcess[,-c(1)], method=c('bagImpute')))
cmp <- predict(cmpImpute, ChemicalManufacturingProcess[,-c(1)])
set.seed(43)
trainRow <- createDataPartition(ChemicalManufacturingProcess$Yield, p=0.8, list=FALSE)
x_train <- cmp[trainRow, ]
y_train <- ChemicalManufacturingProcess$Yield[trainRow]
x_test <- cmp[-trainRow, ]
y_test <- ChemicalManufacturingProcess$Yield[-trainRow]
set.seed(43)
enetFit <- train(x=x_train,
y=y_train,
method='enet',
metric='RMSE',
tuneGrid=expand.grid(.fraction = seq(0, 1, by=0.1),
.lambda = seq(0, 1, by=0.1)),
trControl=trainControl(method='cv'),
preProcess=c('center','scale')
)
enetFit
plot(enetFit)
enet_Pred <- predict(enetFit, newdata=x_test)
(predResult <- postResample(pred=enet_Pred, obs=y_test))
library(elasticnet)
coeffs <- predict.enet(enetFit$finalModel, s=enetFit$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients
coeffs
coeffs.sorted <- abs(coeffs)
coeffs.sorted <- coeffs.sorted[coeffs.sorted>0]
(coeffs.sorted <- sort(coeffs.sorted, decreasing = T))
library(varImp)
(temp <- varImp(enetFit))
library(dplyr)
coeffs_mp <- coeffs.sorted[grep('ManufacturingProcess', names(coeffs.sorted))] %>% names() %>% coeffs[.]
coeffs_mp[coeffs_mp>0]
coeffs_mp[coeffs_mp<0]
library(dplyr)
library(varImp)
library(elasticnet)
library(dplyr)
library(varImp)
library(elasticnet)
install.packages("mlbench")
knitr::opts_chunk$set(echo = TRUE)
library(mlbench)
library(kableExtra)
library(caret)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
knnAccuracy <- postResample(pred = knnPred, obs = testData$y)
SvmRadialModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
SvmRadialModel
nnetGrid <- expand.grid(size = c(1:10),
decay = c(0, 0.01, 0.1),
bag = FALSE)
nnet <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
.size=c(1, 10, 15, 20),
.bag=FALSE)
nnet <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
library(mlbench)
library(kableExtra)
library(caret)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
featurePlot(trainingData$x, trainingData$y)
## or other methods.
## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
knnAccuracy <- postResample(pred = knnPred, obs = testData$y)
SvmRadialModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
SvmRadialModel
svmRadialPred <- predict(SvmRadialModel, newdata = testData$x)
svmRadialAccuracy <- postResample(pred = svmRadialPred, obs = testData$y)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
knnModel
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)
knnAccuracy <- postResample(pred = knnPred, obs = testData$y)
SvmRadialModel <- train(x = trainingData$x,
y = trainingData$y,
method = "svmRadial",
tuneLength=10,
preProc = c("center", "scale"))
SvmRadialModel
svmRadialPred <- predict(SvmRadialModel, newdata = testData$x)
postResample(pred = svmRadialPred, obs = testData$y)
svmRadialAccuracy <- postResample(pred = svmRadialPred, obs = testData$y)
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
.size=c(1, 10, 15, 20),
.bag=FALSE)
nnet <- train(x = trainingData$x,
y = trainingData$y,
method = "avNNet",
tuneGrid = nnetGrid,
preProc = c("center", "scale"),
trace=FALSE,
linout=TRUE,
maxit=500)
nnet
nnetPred <- predict(nnet, newdata = testData$x)
nnetAccuracy <- postResample(pred = nnetPred, obs = testData$y)
nnet
nnetPred <- predict(nnet, newdata = testData$x)
postResample(pred = nnetPred, obs = testData$y)
nnetAccuracy <- postResample(pred = nnetPred, obs = testData$y)
marsGrid <- expand.grid(.degree=1:2,
.nprune=2:20)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
mars
marsGrid <- expand.grid(.degree=1:2,
.nprune=2:20)
marsModel <- train(x = trainingData$x,
y = trainingData$y,
method = "earth",
tuneGrid = marsGrid,
preProc = c("center", "scale"))
marsModel
marsPred <- predict(marsModel, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)
marsAccuracy <- postResample(pred = marsPred, obs = testData$y)
accuracies <- rbind(marsAccuracy,svmRadialAccuracy,knnAccuracy,nnetAccuracy)
rownames(accuracies )<- c("MARS","SVM","KNN", "NeuralNet")
accuracies%>%
kable() %>%
kable_styling()
varImp(marsModel)
install.packages("missForest")
library(AppliedPredictiveModeling)
library(missForest)
library(AppliedPredictiveModeling)
library(Amelia)
library(missForest)
library(AppliedPredictiveModeling)
library(Amelia)
library(missForest)
data(ChemicalManufacturingProcess)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
Original_df <- ChemicalManufacturingProcess
Imputed_df <- missForest(df)
Original_df <- ChemicalManufacturingProcess
Imputed_df <- missForest(Original_df)
df <- Imputed_df$ximp
data <- df[, 2:58]
target <- df[,1]
train <- createDataPartition(target, p=0.75)
train_pred <- data[train$Resample1,]
train_target <- target[train$Resample]
test_pred <- data[-train$Resample1,]
test_target <- target[-train$Resample1]
control <- trainControl(method = "cv", number=10)
library(AppliedPredictiveModeling)
library(Amelia)
library(missForest)
library(AppliedPredictiveModeling)
library(Amelia)
library(missForest)
library(ggplot2)
data(ChemicalManufacturingProcess)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
Original_df <- ChemicalManufacturingProcess
Imputed_df <- missForest(Original_df)
df <- Imputed_df$ximp
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
data <- df[, 2:58]
target <- df[,1]
train <- createDataPartition(target, p=0.75)
train_pred <- data[train$Resample1,]
train_target <- target[train$Resample]
test_pred <- data[-train$Resample1,]
test_target <- target[-train$Resample1]
control <- trainControl(method = "cv", number=10)
set.seed(1)
knnModel <- train(x = train_pred,
y = train_target,
method = "knn",
tuneLength = 10)
knnModel
knn.Predict <- predict(knnModel, newdata = test_pred)
# Use PostResample function
postResample(pred = knn.Predict, obs = test_target)
knnAccuracy <- postResample(pred = knn.Predict, obs = test_target)
set.seed(1)
svmModel <- train(x = train_pred,
y = train_target,
method='svmRadial',
tuneLength=14,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"))
svmModel
svm.Predict <- predict (svmModel, test_pred)
# Use PostResample function
postResample(pred = svm.Predict, obs = test_target)
svmRadialAccuracy <- postResample(pred = svm.Predict, obs = test_target)
set.seed(1)
nnetModel <- nnet(train_pred,
train_target,
size=5,
decay=0.01,
linout= T,
trace=F,
maxit = 500 ,
MaxNWts = 5 * (ncol(train_pred) + 1) + 5 + 1)
library(AppliedPredictiveModeling)
library(Amelia)
library(missForest)
library(nnet)
library(ggplot2)
data(ChemicalManufacturingProcess)
missmap(ChemicalManufacturingProcess, col = c("red", "lightgreen"))
Original_df <- ChemicalManufacturingProcess
Imputed_df <- missForest(Original_df)
df <- Imputed_df$ximp
data <- df[, 2:58]
target <- df[,1]
train <- createDataPartition(target, p=0.75)
train_pred <- data[train$Resample1,]
train_target <- target[train$Resample]
test_pred <- data[-train$Resample1,]
test_target <- target[-train$Resample1]
control <- trainControl(method = "cv", number=10)
set.seed(1)
knnModel <- train(x = train_pred,
y = train_target,
method = "knn",
tuneLength = 10)
knnModel
knn.Predict <- predict(knnModel, newdata = test_pred)
# Use PostResample function
postResample(pred = knn.Predict, obs = test_target)
knnAccuracy <- postResample(pred = knn.Predict, obs = test_target)
set.seed(1)
svmModel <- train(x = train_pred,
y = train_target,
method='svmRadial',
tuneLength=14,
trControl = trainControl(method = "cv"),
preProc = c("center", "scale"))
svmModel
svm.Predict <- predict (svmModel, test_pred)
# Use PostResample function
postResample(pred = svm.Predict, obs = test_target)
svmRadialAccuracy <- postResample(pred = svm.Predict, obs = test_target)
set.seed(1)
nnetModel <- nnet(train_pred,
train_target,
size=5,
decay=0.01,
linout= T,
trace=F,
maxit = 500 ,
MaxNWts = 5 * (ncol(train_pred) + 1) + 5 + 1)
nnetPredict <- predict(nnetModel, test_pred)
# Use PostResample function
postResample(pred = nnetPredict, obs = test_target)
nnetAccuracy <- postResample(pred = nnetPredict, obs = test_target)
set.seed(1)
marsModel <- train(x = train_pred,
y = train_target,
method='earth',
tuneLength=10,
trControl = trainControl(method = "cv"))
marsModel
mars.Predict <- predict (marsModel, test_pred)
# Use PostResample function
postResample(pred = mars.Predict, obs = test_target)
marsAccuracy <- postResample(pred = mars.Predict, obs = test_target)
accuracies <- rbind(marsAccuracy,svmRadialAccuracy,knnAccuracy,nnetAccuracy)
rownames(accuracies )<- c("MARS","SVM","KNN", "NeuralNet")
accuracies%>%
kable() %>%
kable_styling()
accuracies1 <- rbind(marsAccuracy,svmRadialAccuracy,knnAccuracy,nnetAccuracy)
rownames(accuracies)<- c("MARS","SVM","KNN", "NeuralNet")
accuracies1%>%
kable() %>%
kable_styling()
accuracies <- rbind(marsAccuracy,svmRadialAccuracy,knnAccuracy,nnetAccuracy)
rownames(accuracies )<- c("MARS","SVM","KNN", "NeuralNet")
accuracies%>%
kable() %>%
kable_styling()
setwd("E:/github/MS/DATA624")
install.packages("corrgram")
